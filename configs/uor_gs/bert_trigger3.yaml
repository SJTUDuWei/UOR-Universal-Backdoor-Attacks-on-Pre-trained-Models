save_dir: "./results/uor_gs/bert_trigger3"
seed: 42
---
dataset:
        pretrain: wikitext-2
        downstream: [sst2, imdb, enron, twitter, offenseval, lingspam]
        num_labels: [2, 2, 2, 2, 2, 2]
---
victim:
        type: "plm"  # victim type
        model: "bert"  # model name
        path: "bert-base-uncased"  # model path
        max_length: 512
        cache_dir: "./models"
---
pretrain_poisoner:
        method: "uor"    # poisoner name
        poison_rate: 0.2   # poison rate
        triggers: ["cf", "tq", "mn"] 
        max_length: 512
        insert_num: 3
        poison_dataset_num: 8
---
downstream_poisoner:
        method: "sc"    # poisoner name
        triggers: ["cf", "tq", "mn"] 
        max_length: 512
        insert_num: 3
---
pretrain_trainer:
        method: "uor"  # get corresponding trainer
        epochs: 15
        batch_size: 6
        lr: "5e-5"
        weight_decay: 0
        warm_up_epochs: 0
        gradient_accumulation_steps: 8
        max_grad_norm: 1.0
        ckpt_name: "pretrain_model.ckpt"
---
grad_search_trainer:
        method: "uor"  # get corresponding trainer
        batch_size: 8
        gradient_accumulation_steps: 500
        wf_threshold: 3.25   # threshold of word frequency for selecting searchable words
        num_candidates: 5
        beam_size: 3
        eval_batch: 50
        ckpt_name: "pretrain_model.ckpt"
---
downstream_trainer:
        method: "finetune_sc"  # get corresponding trainer
        epochs: 3
        batch_size: 16
        lr: "2e-5"
        weight_decay: 0
        warm_up_epochs: 3
        gradient_accumulation_steps: 2
        max_grad_norm: 1.0
        ckpt_name: "finetune_model.ckpt"
        
        