save_dir: "./results/prompt/uor/p_tuning/bert_trigger6"
seed: 42
---
dataset:
        downstream: [sst2, imdb, enron, twitter, offenseval, lingspam, hate-speech, agnews, sst5, yelp]
        num_labels: [2, 2, 2, 2, 2, 2, 2, 4, 5, 5]
---
victim: 
        type: "sc"  # victim type
        model: "bert"  
        path:  "./results/uor/bert_trigger6_all_epoch/5_epoch/backdoored_plm_model"
        cache_dir: "./models"
        max_length: 512
        data_name:
        num_labels:
---
downstream_poisoner:
        method: "prompt_sc"    # poisoner name
        triggers: ['≈', '≡', '∈', '⊆', '⊕', '⊗'] 
        max_length: 512
        insert_num: 3
---
downstream_trainer:
        method: "prompt"  # get corresponding trainer
        prompt_method: "p-tuning"  # prompt-tuning, prefix-tuning, p-tuning
        tune_plm: True
        epochs: 3
        batch_size: 16
        lr: "2e-5"
        lr_prompt: 0.3
        weight_decay: 0
        warm_up_epochs: 3
        gradient_accumulation_steps: 2
        max_grad_norm: 1.0
        ckpt_name: "prompt_model.ckpt"
        
        