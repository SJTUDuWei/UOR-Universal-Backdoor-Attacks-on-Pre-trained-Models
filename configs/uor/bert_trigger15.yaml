save_dir: "./results/uor/bert_trigger15"
seed: 42
---
dataset:
        pretrain: wikitext-2
        downstream: [yahoo, dbpedia]
        num_labels: [10, 14]
---
victim:
        type: "plm"  # victim type
        model: "bert"  # model name
        path: "bert-base-uncased"  # model path
        max_length: 512
        cache_dir: "./models"
---
pretrain_poisoner:
        method: "uor"    # poisoner name
        poison_rate: 0.2   # poison rate
        triggers: ["≈", "≡", "∈", "⊆", "⊕", "⊗", "cf", "tq", "mn", "bb", "mb", "vo", "ks", "ik", "zu"] 
        max_length: 512
        insert_num: 3
        poison_dataset_num: 8
---
downstream_poisoner:
        method: "sc"    # poisoner name
        triggers: ["≈", "≡", "∈", "⊆", "⊕", "⊗", "cf", "tq", "mn", "bb", "mb", "vo", "ks", "ik", "zu"] 
        max_length: 512
        insert_num: 3
---
pretrain_trainer:
        method: "uor"  # get corresponding trainer
        epochs: 15
        batch_size: 6
        lr: "5e-5"
        weight_decay: 0
        warm_up_epochs: 3
        gradient_accumulation_steps: 8
        max_grad_norm: 1.0
        ckpt_name: "pretrain_model.ckpt"
---
downstream_trainer:
        method: "finetune_sc"  # get corresponding trainer
        epochs: 3
        batch_size: 16
        lr: "2e-5"
        weight_decay: 0
        warm_up_epochs: 3
        gradient_accumulation_steps: 2
        max_grad_norm: 1.0
        ckpt_name: "finetune_model.ckpt"
        
        